{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c059077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pronn\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pronn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tabulate import tabulate\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f654192",
   "metadata": {},
   "source": [
    "### Word Frequency Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5175a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a text: nory was a catholic because her mother , and 's father his or had been\n",
      "Word  Frequency Count\n",
      "nory        1\n",
      "was         1\n",
      "a           1\n",
      "catholic    1\n",
      "because     1\n",
      "her         1\n",
      "mother      1\n",
      ",           1\n",
      "and         1\n",
      "'s          1\n",
      "father      1\n",
      "his         1\n",
      "or          1\n",
      "had         1\n",
      "been        1\n",
      "dtype: int64\n",
      "['nory', 'was', 'a', 'catholic', 'because', 'her', 'mother', ',', 'and', \"'s\", 'father', 'his', 'or', 'had', 'been'] ['his', 'mother', 'or', 'was', 'because', ',', \"'s\", 'nory', 'and', 'a', 'had', 'father', 'been', 'catholic', 'her']\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Enter a text: \")\n",
    "\n",
    "sentence = nltk.sent_tokenize(text)\n",
    "tokens = [nltk.word_tokenize(sentence) for sentence in sentence]\n",
    "\n",
    "print(\"Word  Frequency Count\")\n",
    "for sentence_tokens in tokens :\n",
    "    word_counts = pd.value_counts(np.array(sentence_tokens))\n",
    "    print(word_counts)\n",
    "\n",
    "flat_tokens = [word.lower() for sentence_tokens in tokens for word in sentence_tokens]\n",
    "unique_words = list(set(flat_tokens))\n",
    "\n",
    "print (flat_tokens, unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0d84c6",
   "metadata": {},
   "source": [
    "### Bigram Count Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "920b1404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigram Count Table:\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "|          |   his |   mother |   or |   was |   because |   , |   's |   nory |   and |   a |   had |   father |   been |   catholic |   her |\n",
      "+==========+=======+==========+======+=======+===========+=====+======+========+=======+=====+=======+==========+========+============+=======+\n",
      "| his      |     0 |        0 |    1 |     0 |         0 |   0 |    0 |      0 |     0 |   0 |     0 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| mother   |     0 |        0 |    0 |     0 |         0 |   1 |    0 |      0 |     0 |   0 |     0 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| or       |     0 |        0 |    0 |     0 |         0 |   0 |    0 |      0 |     0 |   0 |     1 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| was      |     0 |        0 |    0 |     0 |         0 |   0 |    0 |      0 |     0 |   1 |     0 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| because  |     0 |        0 |    0 |     0 |         0 |   0 |    0 |      0 |     0 |   0 |     0 |        0 |      0 |          0 |     1 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| ,        |     0 |        0 |    0 |     0 |         0 |   0 |    0 |      0 |     1 |   0 |     0 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| 's       |     0 |        0 |    0 |     0 |         0 |   0 |    0 |      0 |     0 |   0 |     0 |        1 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| nory     |     0 |        0 |    0 |     1 |         0 |   0 |    0 |      0 |     0 |   0 |     0 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| and      |     0 |        0 |    0 |     0 |         0 |   0 |    1 |      0 |     0 |   0 |     0 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| a        |     0 |        0 |    0 |     0 |         0 |   0 |    0 |      0 |     0 |   0 |     0 |        0 |      0 |          1 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| had      |     0 |        0 |    0 |     0 |         0 |   0 |    0 |      0 |     0 |   0 |     0 |        0 |      1 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| father   |     1 |        0 |    0 |     0 |         0 |   0 |    0 |      0 |     0 |   0 |     0 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| been     |     0 |        0 |    0 |     0 |         0 |   0 |    0 |      0 |     0 |   0 |     0 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| catholic |     0 |        0 |    0 |     0 |         1 |   0 |    0 |      0 |     0 |   0 |     0 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| her      |     0 |        1 |    0 |     0 |         0 |   0 |    0 |      0 |     0 |   0 |     0 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n"
     ]
    }
   ],
   "source": [
    "bigrams = list(nltk.bigrams(flat_tokens))\n",
    "unigram_counter = Counter(flat_tokens)\n",
    "matrix_size = len(unique_words)\n",
    "matrix = [[0] * matrix_size for _ in range(matrix_size)]\n",
    "word_to_index = {word : index for index, word in enumerate(unique_words)}\n",
    "\n",
    "for bigram in bigrams :\n",
    "    word1, word2 = bigram\n",
    "    index1 = word_to_index[word1]\n",
    "    index2 = word_to_index[word2]\n",
    "    matrix[index1][index2] += 1\n",
    "\n",
    "header = [''] + unique_words\n",
    "matrix_with_headers = [[unique_words[i]] + matrix[i] for i in range(matrix_size)]\n",
    "\n",
    "print(\"\\nBigram Count Table:\")\n",
    "print(tabulate(matrix_with_headers, headers=header, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e347128",
   "metadata": {},
   "source": [
    "### Bigram Probability table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a67d078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigram Probability Table:\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "|          |   his |   mother |   or |   was |   because |   , |   's |   nory |   and |   a |   had |   father |   been |   catholic |   her |\n",
      "+==========+=======+==========+======+=======+===========+=====+======+========+=======+=====+=======+==========+========+============+=======+\n",
      "| his      |     0 |        0 |    1 |     0 |         0 |   0 |    0 |      0 |     0 |   0 |     0 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| mother   |     0 |        0 |    0 |     0 |         0 |   1 |    0 |      0 |     0 |   0 |     0 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| or       |     0 |        0 |    0 |     0 |         0 |   0 |    0 |      0 |     0 |   0 |     1 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| was      |     0 |        0 |    0 |     0 |         0 |   0 |    0 |      0 |     0 |   1 |     0 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| because  |     0 |        0 |    0 |     0 |         0 |   0 |    0 |      0 |     0 |   0 |     0 |        0 |      0 |          0 |     1 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| ,        |     0 |        0 |    0 |     0 |         0 |   0 |    0 |      0 |     1 |   0 |     0 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| 's       |     0 |        0 |    0 |     0 |         0 |   0 |    0 |      0 |     0 |   0 |     0 |        1 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| nory     |     0 |        0 |    0 |     1 |         0 |   0 |    0 |      0 |     0 |   0 |     0 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| and      |     0 |        0 |    0 |     0 |         0 |   0 |    1 |      0 |     0 |   0 |     0 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| a        |     0 |        0 |    0 |     0 |         0 |   0 |    0 |      0 |     0 |   0 |     0 |        0 |      0 |          1 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| had      |     0 |        0 |    0 |     0 |         0 |   0 |    0 |      0 |     0 |   0 |     0 |        0 |      1 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| father   |     1 |        0 |    0 |     0 |         0 |   0 |    0 |      0 |     0 |   0 |     0 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| been     |     0 |        0 |    0 |     0 |         0 |   0 |    0 |      0 |     0 |   0 |     0 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| catholic |     0 |        0 |    0 |     0 |         1 |   0 |    0 |      0 |     0 |   0 |     0 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n",
      "| her      |     0 |        1 |    0 |     0 |         0 |   0 |    0 |      0 |     0 |   0 |     0 |        0 |      0 |          0 |     0 |\n",
      "+----------+-------+----------+------+-------+-----------+-----+------+--------+-------+-----+-------+----------+--------+------------+-------+\n"
     ]
    }
   ],
   "source": [
    "bigrams = list(nltk.bigrams(flat_tokens))\n",
    "unigram_counter = Counter(flat_tokens)\n",
    "matrix_size = len(unique_words)\n",
    "matrix = [[0] * matrix_size for _ in range(matrix_size)]\n",
    "word_to_index = {word : index for index, word in enumerate(unique_words)}\n",
    "\n",
    "for bigram in bigrams :\n",
    "    word1, word2 = bigram\n",
    "    index1 = word_to_index[word1]\n",
    "    index2 = word_to_index[word2]\n",
    "    matrix[index1][index2] += 1 / unigram_counter[word1]\n",
    "\n",
    "header = [''] + unique_words\n",
    "matrix_with_headers = [[unique_words[i]] + matrix[i] for i in range(matrix_size)]\n",
    "\n",
    "print(\"\\nBigram Probability Table:\")\n",
    "print(tabulate(matrix_with_headers, headers=header, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ea7423",
   "metadata": {},
   "source": [
    "### Predicting next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e9c845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a starting word: mother\n",
      "Predicted next word: ,\n"
     ]
    }
   ],
   "source": [
    "def predict_next_word(start_word, matrix_with_headers, word_to_index):\n",
    "    if start_word in word_to_index :\n",
    "        start_index = word_to_index[start_word]\n",
    "        next_word_probs = matrix_with_headers[start_index][1:]\n",
    "        max_prob_index = next_word_probs.index(max(next_word_probs))\n",
    "        return unique_words[max_prob_index]\n",
    "    else:\n",
    "        return \"Starting word not found in the text.\"\n",
    "\n",
    "starting_word = input(\"Enter a starting word: \")\n",
    "predicted_next_word = predict_next_word(starting_word, matrix_with_headers, word_to_index)\n",
    "print(\"Predicted next word:\", predicted_next_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
